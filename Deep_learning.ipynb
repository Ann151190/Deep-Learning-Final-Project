{"cells":[{"source":[],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":[""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","import os\n","import numpy as np\n","import pandas as pd\n","import sklearn as sk\n","from scipy import stats\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","os.chdir(\"/Users/ananyaneogi/Desktop/FS-SECOND SEM/Deep learning Project/Homework_Assignment\")\n","df1 = pd.read_csv('solditems_encoded_stage2.csv')\n","df2 = pd.read_csv('content_encoded_stage2.csv')\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","print(\"Solid Item dataframe shape :\", df1.shape)\n","print(\"Solid Item dataframe columns :\",df1.columns)\n","df1.head(5)\n","\n","#A copy of the existing dataframe made for alteration purposes\n","df1_new = df1.copy()\n","\n","# Type of the date id changed to datetime format\n","df1_new['created_date_id'] = pd.to_datetime(df1_new['created_date_id'])\n","\n","# From the date id the date field is extracted for further grouping purposes\n","df1_new['year'] = df1_new['created_date_id'].dt.year\n","df1_new.head(5)\n","df1_new.groupby('product_sid').apply(lambda x: x.sort_values(['year'], ascending = True))\n","\n","# Total units sold of a product for every year.\n","product_count = df1_new.groupby(['year', 'product_sid']).size()\n","\n","print(\"Content dataframe shape :\", df2.shape)\n","print(\"Content dataframe columns :\",df2.columns)\n","df2.describe()\n","pd.set_option('display.max_rows', 1000)\n","\n","manufacturers_to_drop = ['Manufacturer_1','Manufacturer_2','Manufacturer_9','Manufacturer_4','Manufacturer_5',\n","                        'Manufacturer_6','Manufacturer_7','Manufacturer_8','Manufacturer_10','Manufacturer_11',\n","                        'Manufacturer_12','Manufacturer_13','Manufacturer_15','Manufacturer_16','Manufacturer_17',\n","                        'Manufacturer_18','Manufacturer_19','Manufacturer_20','Manufacturer_21','Manufacturer_22',\n","                        'Manufacturer_23','Manufacturer_24','Manufacturer_25','Manufacturer_26','Manufacturer_27',\n","                        'Manufacturer_28','Manufacturer_29','Manufacturer_30','Manufacturer_31','Manufacturer_32',\n","                        'Manufacturer_34','Manufacturer_35','Manufacturer_36','Manufacturer_37','Manufacturer_38',\n","                        'Manufacturer_39','Manufacturer_40','Manufacturer_41','Manufacturer_42','Manufacturer_43',\n","                        'Manufacturer_44','Manufacturer_45','Manufacturer_46','Manufacturer_47','Manufacturer_48',\n","                        'Manufacturer_49','Manufacturer_50','Manufacturer_51','Manufacturer_52','Manufacturer_53',\n","                        'Manufacturer_54','Manufacturer_55','Manufacturer_56','Manufacturer_57','Manufacturer_58',\n","                        'Manufacturer_59','Manufacturer_60','Manufacturer_61','Manufacturer_62','Manufacturer_63',\n","                        'Manufacturer_64','Manufacturer_65','Manufacturer_66','Manufacturer_67','Manufacturer_68',\n","                        'Manufacturer_69','Manufacturer_70','Manufacturer_71','Manufacturer_72','Manufacturer_73',\n","                        'Manufacturer_74']\n","df2 = df2.drop(manufacturers_to_drop, axis = 1)\n","df2.shape\n","\n","count_rows = df2.apply(lambda x: True if x['Manufacturer_0'] or x['Manufacturer_3'] or x['Manufacturer_14'] or x['Manufacturer_33'] == 1 else False , axis=1)\n","num_rows = len(count_rows[count_rows == True].index)\n","print('Number of Rows where value is 1 for the manufacturers are: ', num_rows)\n","\n","df2 = df2[df2['Manufacturer_0'] | df2['Manufacturer_3'] | df2['Manufacturer_14'] | df2['Manufacturer_33'] == 1] \n","df2.shape\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","df3 = pd.merge(df1, df2, left_on=\"product_sid\", right_on=\"ProductId\")\n","print(df3.head())\n","print(df3.shape)\n","pd.set_option('display.max_rows', 1000)\n","print(df3.isna().sum())\n","\n","print(df3.describe())\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","from statsmodels.graphics.tsaplots import plot_pacf\n","\n","#columns = [] #use this for speedup\n","columns = [\"sales_item_price\", 'sales_voucher_created', 'sales_voucher',\n","       'sales_value_created', 'sales_value','created_date_id', 'sales_item_price_created',\n","       'days_since_first_sold', 'days_since_release', 'returned_date_id_0',\n","       'returned_date_id_1']\n","\n","for col in columns:\n","    plt.figure()\n","    plot_pacf(df1_new[col].dropna(), lags=48, zero=False)\n","    plt.title(\"Partial Autocorrelation PLot : \" + str(col))\n","    \n","plt.show()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","from statsmodels.graphics.tsaplots import _prepare_data_corr_plot, _plot_corr\n","import statsmodels.graphics.utils as utils\n","from statsmodels.tsa.stattools import pacf\n","\n","def plot_pacf_drop(x, ax=None, lags=None, alpha=.05, method='ywunbiased',\n","              use_vlines=True, title='Partial Autocorrelation', zero=True,\n","              vlines_kwargs=None, drop_no=0, **kwargs):\n","    \n","    lags_orig=lags\n","    fig, ax = utils.create_mpl_ax(ax)\n","    vlines_kwargs = {} if vlines_kwargs is None else vlines_kwargs\n","    lags, nlags, irregular = _prepare_data_corr_plot(x, lags, zero)\n","    confint = None\n","    if alpha is None:\n","        acf_x = pacf(x, nlags=nlags, alpha=alpha, method=method)\n","    else:\n","        acf_x, confint = pacf(x, nlags=nlags, alpha=alpha, method=method)\n","\n","    if drop_no:\n","        acf_x = acf_x[drop_no+1:]\n","        confint = confint[drop_no+1:]\n","        lags, nlags, irregular = _prepare_data_corr_plot(x, lags_orig-drop_no, zero)\n","        \n","    _plot_corr(ax, title, acf_x, confint, lags, False, use_vlines,\n","               vlines_kwargs, **kwargs)\n","\n","    return fig\n","\n","    import matplotlib.pyplot as plt\n","\n","#columns = [] #use this for speedup\n","columns = [\"sales_item_price\", 'sales_voucher_created', 'sales_voucher',\n","       'sales_value_created', 'sales_value','created_date_id', 'sales_item_price_created',\n","       'days_since_first_sold', 'days_since_release', 'returned_date_id_0',\n","       'returned_date_id_1']\n","\n","for col in columns:\n","\n","    plt.figure()\n","    plot_pacf_drop(df1_new[col].dropna(), lags=200, drop_no=3, zero=False)\n","    plt.title(\"Partial Autocorrelation PLot : \" + str(col))\n","    \n","plt.show()\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","#grouped = df1.groupby(['product_sid'])\n","#l_grouped = list(grouped)\n","#l_grouped[0][1]\n","\n","#l_grouped.product_sid.unique()\n","\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","from seglearn.split import temporal_split\n","\n","X_train, X_else, y_train, y_else = train_test_split(df1_new, df1_new[\"sales_item_price_created\"], test_size=0.2, shuffle=False)\n","X_valid, X_test, y_valid, y_test = train_test_split(X_else, y_else, test_size=0.5, shuffle=False)\n","\n","#X_train, X_valid, y_train, y_valid = temporal_split(df1_new, df1_new[\"sales_item_price_created\"], test_size=0.25)\n","\n","#normalizers = minmax_scale(X_train, y_train)"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","TIME_WINDOW=100\n","FORECAST_DISTANCE=24\n","\n","from seglearn.transform import FeatureRep, SegmentXYForecast, last\n","\n","segmenter = SegmentXYForecast(width=TIME_WINDOW, step=1, y_func=last, forecast=FORECAST_DISTANCE)\n","\n","X_train_rolled, y_train_rolled,_=segmenter.fit_transform([X_train.values],[y_train])\n","\n","X_train_rolled\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","X_train_rolled.shape\n","\n","shape = X_train_rolled.shape\n","X_train_flattened = X_train_rolled.reshape(shape[0],shape[1]*shape[2])\n","X_train_flattened.shape\n","\n","\n","X_valid_rolled, y_valid_rolled,_=segmenter.fit_transform([X_valid.values],[y_valid])\n","\n","shape = X_valid_rolled.shape\n","X_valid_flattened = X_valid_rolled.reshape(shape[0],shape[1]*shape[2])\n","\n","X_valid_flattened"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","from sklearn.metrics import mean_squared_error\n","from math import sqrt\n","from sklearn.preprocessing import MinMaxScaler\n","import warnings\n","from sklearn.exceptions import DataConversionWarning\n","\n","def evaluate_model(model, X_valid, y_valid_true):\n","    predictions = model.predict(X_valid)\n","    rms = sqrt(mean_squared_error(y_valid_true, predictions))\n","    print(\"Root mean squared error on valid:\",rms)\n","    #normalized_rms = df1_new[\"sales_item_price_created\"].inverse_transform(np.array([rms]).reshape(1, -1))[0][0]\n","    #print(\"Root mean squared error on valid inverse transformed from normalization:\",normalized_rms)\n","    return rms\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","from sklearn.dummy import DummyRegressor\n","\n","dummy_model = DummyRegressor(strategy=\"mean\", constant=None, quantile=None)\n","\n","dummy_model.fit(X_train_flattened,y_train_rolled)\n","\n","result = evaluate_model(dummy_model,X_valid_flattened,y_valid_rolled)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","import xgboost as xgb\n","# If in trouble, use !pip install xgboost\n","\n","# XGBoost needs it's custom data format to run quickly\n","dmatrix_train = xgb.DMatrix(data=X_train_flattened,label=y_train_rolled)\n","dmatrix_valid = xgb.DMatrix(data=X_valid_flattened,label=y_valid_rolled)\n","\n","params = {'objective': 'reg:linear', 'eval_metric': 'rmse', 'n_estimators': 20}\n","\n","evallist = [(dmatrix_valid, 'eval'), (dmatrix_train, 'train')]\n","\n","num_round = 10 #Can easily overfit, experiment with it!\n","\n","xg_reg = xgb.train(params, dmatrix_train, num_round,evallist)\n","\n","result = evaluate_model(xg_reg,dmatrix_valid,y_valid_rolled)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["\n","LSTM_CELL_SIZE=350\n","BATCH_SIZE = 300\n","EPOCHS = 60\n","DROPOUT_RATE=0\n","\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, CuDNNLSTM\n","from tensorflow.keras import backend as be\n","from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n","\n","column_count=len(X_train.columns) #Remember,column count before rolling...\n","\n","be.clear_session()\n","\n","# You might very well be needing it!\n","# Remeber to save only what is worth it from validation perspective...\n","# model_saver = ModelCheckpoint(...)\n","\n","# If you need it...\n","#def schedule(epoch, lr):\n","#    ...\n","#    return lr\n","\n","#lr_scheduler = LearningRateScheduler(schedule)\n","\n","# Build your whole LSTM model here!\n","model = Sequential()\n","\n","model.add(CuDNNLSTM(LSTM_CELL_SIZE, input_shape=(TIME_WINDOW,column_count),stateful=False))\n","model.add(Dense(1, activation= \"linear\"))\n","\n","\n","#For shape remeber, we have a variable defining the \"window\" and the features in the window...\n","\n","model.compile(loss='mean_squared_error', optimizer='sgd')\n","# Fit on the train data\n","# USE the batch size parameter!\n","# Use validation data - warning, a tuple of stuff!\n","# Epochs as deemed necessary...\n","# You should avoid shuffling the data maybe.\n","# You can use the callbacks for LR schedule or model saving as seems fit.\n","history = model.fit(X_train_rolled, y_train_rolled, batch_size=BATCH_SIZE, epochs=EPOCHS,\n","          validation_data=(X_valid_rolled ,y_valid_rolled), shuffle=False)\n","\n","# Plot the loss function of training and test sets\n","plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='test')\n","plt.legend()\n","plt.show()\n","\n","result = evaluate_model(model,X_valid_rolled ,y_valid_rolled)"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}